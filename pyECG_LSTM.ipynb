{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming your CSV files are in a 'data' directory relative to your script\n",
    "data_dir = Path('csvdata')\n",
    "print(data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ecg_file, labels_file):\n",
    "    ecg_signal = pd.read_csv(ecg_file, header=None).values.flatten()\n",
    "    labels = pd.read_csv(labels_file)\n",
    "    return ecg_signal, labels\n",
    "\n",
    "def create_mask(ecg_signal, labels):\n",
    "    mask = np.zeros(len(ecg_signal), dtype=int)\n",
    "    for _, row in labels.iterrows():\n",
    "        start, end = row['ROILimits_1'], row['ROILimits_2']\n",
    "        value = row['Value']\n",
    "        mask[start:end] = ['P', 'QRS', 'T'].index(value) + 1\n",
    "    return mask\n",
    "\n",
    "def resize_data(ecg_signal, mask, segment_length=5000):\n",
    "    num_segments = len(ecg_signal) // segment_length\n",
    "    ecg_segments = np.array_split(ecg_signal[:num_segments*segment_length], num_segments)\n",
    "    mask_segments = np.array_split(mask[:num_segments*segment_length], num_segments)\n",
    "    return np.array(ecg_segments), np.array(mask_segments)\n",
    "\n",
    "# Passing very long input signals into the LSTM network can result in estimation performance degradation and excessive memory usage. \n",
    "# To avoid these effects, break the ECG signals and their corresponding label masks.\n",
    "# create as many 5000-sample segments as possible and discard the remaining samples. \n",
    "\n",
    "# Normalise data\n",
    "def normalise_signal(signal):\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "\n",
    "# Initialize empty lists:    \n",
    "all_ecg_segments_norm = []\n",
    "all_mask_segments_norm = []\n",
    "#These lists will store the processed ECG segments and their corresponding masks.\n",
    "\n",
    "# Load and process all files\n",
    "\n",
    "for i in range(1, 211):  # Assuming 210 files\n",
    "    ecg_file = data_dir / f'ecg{i}_ecgSignal.csv'\n",
    "    labels_file = data_dir / f'ecg{i}_signalRegionLabels.csv'\n",
    "    \n",
    "    ecg_signal, labels = load_data(ecg_file, labels_file)\n",
    "    ecg_signal_norm = normalise_signal(ecg_signal)\n",
    "    mask = create_mask(ecg_signal_norm, labels)\n",
    "    ecg_segments, mask_segments = resize_data(ecg_signal_norm, mask)\n",
    "    \n",
    "    all_ecg_segments_norm.extend(ecg_segments)  \n",
    "    all_mask_segments_norm.extend(mask_segments)\n",
    "\n",
    "# The processed segments are added to the respective lists.\n",
    "\n",
    "# Convert to numpy arrays:\n",
    "X_norm = np.array(all_ecg_segments_norm)\n",
    "y_norm = np.array(all_mask_segments_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array verification\n",
    "\n",
    "ecg1_signal = pd.read_csv(data_dir/'ecg1_ecgSignal.csv', header=None).values.flatten()\n",
    "ecg1_labels = pd.read_csv(data_dir/'ecg1_signalRegionLabels.csv')\n",
    "print(type(ecg1_signal))\n",
    "print(type(ecg1_labels))\n",
    "print(ecg1_signal[:5])\n",
    "print(ecg1_signal.ndim)\n",
    "print(ecg1_signal.shape)\n",
    "print(ecg1_labels.head())\n",
    "\n",
    "ecg1_signal_norm = (ecg1_signal - np.mean(ecg1_signal)) / np.std(ecg1_signal)\n",
    "print(type(ecg1_signal_norm))\n",
    "print(ecg1_signal_norm[:5])\n",
    "print(ecg1_signal_norm.ndim)\n",
    "print(ecg1_signal_norm.shape)\n",
    "\n",
    "print(\"all_ecg_segments_norm list length:\", len(all_ecg_segments_norm))\n",
    "print(type(all_ecg_segments_norm))\n",
    "print(\"all_mask_segments_norm length:\", len(all_mask_segments_norm))\n",
    "print(type(all_mask_segments_norm))\n",
    "print(type(X_norm))\n",
    "print(\"X_norm shape:\", X_norm.shape)\n",
    "print(type(y_norm))\n",
    "print(\"y_norm shape:\", y_norm.shape)\n",
    "print(\"X_norm\", X_norm[:5])\n",
    "print(\"y_norm\", y_norm[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the data into training and testing sets\n",
    "\n",
    "test_size=0.3: This parameter specifies that 30% of the data should be allocated to the test set, while the remaining 70% will be used for training\n",
    "\n",
    "random_state=42: This parameter sets a seed for the random number generator, ensuring that the split is reproducible. Using the same random_state will always produce the same split, which is crucial for reproducibility in machine learning experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_train, X_norm_test, y_norm_train, y_norm_test = train_test_split(X_norm, y_norm, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Prepare the data for LSTM\n",
    "\n",
    "## one-hot encoding for categorical labels\n",
    "One-hot encoding is a technique used to convert categorical variables into a format suitable for machine learning algorithms. It transforms categorical data into a binary representation, allowing models to process and interpret non-numeric information effectively.\n",
    "How One-Hot Encoding Works\n",
    "The process of one-hot encoding involves the following steps:\n",
    "\t1.\tIdentify unique categories within a categorical variable.\n",
    "\t2.\tCreate new binary columns, one for each unique category.\n",
    "\t3.\tFor each data point, assign a value of 1 in the column corresponding to its category and 0 in all other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm_train_cat = to_categorical(y_norm_train)\n",
    "y_norm_test_cat= to_categorical(y_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array dimension analysis\n",
    "\n",
    "print(\"y_norm_train shape:\", type(y_norm_train_cat), y_norm_train_cat.shape)\n",
    "print(\"y_norm_train shape:\", type(y_norm_train), y_norm_train.shape)\n",
    "print(\"X_norm_train shape:\", X_norm_train.shape)\n",
    "print(\"y_norm_train_cat shape:\", y_norm_train_cat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create and compile the LSTM model\n",
    "\n",
    "Selection of Layer Sizes\n",
    "\t1.\t64 units in the first LSTM layer:\n",
    "\t•\tThis is likely chosen to capture a rich set of features from the input sequence.\n",
    "\t•\tA larger number allows for more complex pattern recognition.\n",
    "\t2.\t32 units in the second LSTM layer:\n",
    "\t•\tReduction in units helps in distilling the most important features.\n",
    "\t•\tIt’s common to reduce the number of units in deeper layers to prevent overfitting.\n",
    "\t3.\t5 neurons in the output layer:\n",
    "\t•\tThis directly corresponds to the number of classes in the classification task.\n",
    "\t•\tEach neuron represents the probability of the input belonging to one of the 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Input, TimeDistributed\n",
    "\n",
    "model_norm = Sequential([\n",
    "    Input(shape=(5000, 1)),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Bidirectional(LSTM(32, return_sequences=True)),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model_norm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train the model\n",
    "\n",
    "Key Components\n",
    "\t1.\tmodel.fit(): This is the Keras method used to train the model on data.\n",
    "\t2.\tX_train: The input training data. This typically contains the features or sequences the model will learn from.\n",
    "\t3.\ty_train_cat: The target labels for the training data. The ‘_cat’ suffix suggests these are categorical (one-hot encoded) labels.\n",
    "\t4.\tvalidation_split=0.2: This parameter sets aside 20% of the training data for validation. The model won’t train on this data but will use it to evaluate performance after each epoch.\n",
    "\t5.\tepochs=10: The number of times the model will iterate over the entire training dataset. Here, it’s set to 10 complete passes.\n",
    "\t6.\tbatch_size=32: This defines how many samples the model will process before updating its internal parameters. A batch size of 32 is a common choice, balancing between computational efficiency and model update frequency.\n",
    "\t7.\thistory: The variable that stores the output of the training process. It contains information about the training metrics (like loss and accuracy) for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_norm.fit(X_norm_train, y_norm_train_cat, validation_split=0.2, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_norm.evaluate(X_norm_test, y_norm_test_cat)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Input shape:\", model_norm.input_shape)\n",
    "print(\"Output shape:\", model_norm.output_shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py_ecg)",
   "language": "python",
   "name": "py_ecg"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
