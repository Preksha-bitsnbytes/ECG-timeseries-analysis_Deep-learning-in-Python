{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Data loading and preprocessing functions (as provided)\n",
    "def load_data(ecg_file, labels_file):\n",
    "    ecg_signal = pd.read_csv(ecg_file, header=None).values.flatten()\n",
    "    labels = pd.read_csv(labels_file)\n",
    "    return ecg_signal, labels\n",
    "\n",
    "def create_mask(ecg_signal, labels):\n",
    "    mask = np.zeros(len(ecg_signal), dtype=int)\n",
    "    for _, row in labels.iterrows():\n",
    "        start, end = row['ROILimits_1'], row['ROILimits_2']\n",
    "        value = row['Value']\n",
    "        mask[start:end] = ['P', 'QRS', 'T'].index(value) + 1\n",
    "    return mask\n",
    "\n",
    "def resize_data(ecg_signal, mask, segment_length=5000):\n",
    "    num_segments = len(ecg_signal) // segment_length\n",
    "    ecg_segments = np.array_split(ecg_signal[:num_segments*segment_length], num_segments)\n",
    "    mask_segments = np.array_split(mask[:num_segments*segment_length], num_segments)\n",
    "    return np.array(ecg_segments), np.array(mask_segments)\n",
    "\n",
    "# CNN model definition\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(32, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = Path('path/to/your/data/directory')\n",
    "    \n",
    "    # Load and process all files\n",
    "    all_ecg_segments = []\n",
    "    all_mask_segments = []\n",
    "\n",
    "    for i in range(1, 211):  # Assuming 210 files\n",
    "        ecg_file = data_dir / f'ecg{i}_ecgSignal.csv'\n",
    "        labels_file = data_dir / f'ecg{i}_signalRegionLabels.csv'\n",
    "        \n",
    "        ecg_signal, labels = load_data(ecg_file, labels_file)\n",
    "        mask = create_mask(ecg_signal, labels)\n",
    "        ecg_segments, mask_segments = resize_data(ecg_signal, mask)\n",
    "        \n",
    "        all_ecg_segments.extend(ecg_segments)\n",
    "        all_mask_segments.extend(mask_segments)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(all_ecg_segments)\n",
    "    y = np.array(all_mask_segments)\n",
    "\n",
    "    # Reshape X to add channel dimension\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define model parameters\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    num_classes = 4  # Background, P, QRS, T\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = create_cnn_model(input_shape, num_classes)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Plot results for a sample\n",
    "    sample_idx = 0\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X_test[sample_idx].flatten())\n",
    "    plt.title(\"ECG Signal with Predicted Segments\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "\n",
    "    # Add colored regions for predicted segments\n",
    "    colors = ['gray', 'red', 'green', 'blue']\n",
    "    labels = ['Background', 'P', 'QRS', 'T']\n",
    "    for i, label in enumerate(labels):\n",
    "        mask = predictions[sample_idx].argmax(axis=1) == i\n",
    "        plt.fill_between(range(len(mask)), X_test[sample_idx].flatten(), where=mask, alpha=0.3, color=colors[i], label=label)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing functions (as provided)\n",
    "def load_data(ecg_file, labels_file):\n",
    "    ecg_signal = pd.read_csv(ecg_file, header=None).values.flatten()\n",
    "    region_labels = pd.read_csv(labels_file)\n",
    "    return ecg_signal, region_labels\n",
    "\n",
    "def create_mask(ecg_signal, region_labels):\n",
    "    mask = np.zeros(len(ecg_signal), dtype=int)\n",
    "    for _, row in region_labels.iterrows():\n",
    "        start, end = row['ROILimits_1'], row['ROILimits_2']\n",
    "        value = row['Value']\n",
    "        mask[start:end] = ['P', 'QRS', 'T'].index(value) + 1\n",
    "    return mask\n",
    "\n",
    "def resize_data(ecg_signal, mask, segment_length=5000):\n",
    "    num_segments = len(ecg_signal) // segment_length\n",
    "    ecg_segments = np.array_split(ecg_signal[:num_segments*segment_length], num_segments)\n",
    "    mask_segments = np.array_split(mask[:num_segments*segment_length], num_segments)\n",
    "    return np.array(ecg_segments), np.array(mask_segments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csvdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming your CSV files are in a 'data' directory relative to your script\n",
    "data_dir = Path('csvdata')\n",
    "print(data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process all files\n",
    "all_ecg_segments = []\n",
    "all_mask_segments = []\n",
    "\n",
    "for i in range(1, 211):  # Assuming 210 files\n",
    "    ecg_file = data_dir / f'ecg{i}_ecgSignal.csv'\n",
    "    labels_file = data_dir / f'ecg{i}_signalRegionLabels.csv'\n",
    "        \n",
    "    ecg_signal, region_labels = load_data(ecg_file, labels_file)\n",
    "    mask = create_mask(ecg_signal, region_labels)\n",
    "    ecg_segments, mask_segments = resize_data(ecg_signal, mask)\n",
    "        \n",
    "    all_ecg_segments.extend(ecg_segments)\n",
    "    all_mask_segments.extend(mask_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = np.array(all_ecg_segments)\n",
    "y = np.array(all_mask_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (9346, 5000) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, OneHotEncoder\n\u001b[1;32m      3\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m----> 4\u001b[0m integer_labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m onehot_encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder()\u001b[38;5;241m.\u001b[39mset_params(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m onehot_labels \u001b[38;5;241m=\u001b[39m onehot_encoder\u001b[38;5;241m.\u001b[39mfit_transform(integer_labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/PG_Github/MFA-matpy/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/Desktop/PG_Github/MFA-matpy/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1406\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1396\u001b[0m             (\n\u001b[1;32m   1397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1402\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1403\u001b[0m         )\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m-> 1406\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1408\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (9346, 5000) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_labels = label_encoder.fit_transform(y)\n",
    "\n",
    "onehot_encoder = OneHotEncoder().set_params(sparse_output=False)\n",
    "onehot_labels = onehot_encoder.fit_transform(integer_labels.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9346, 5000)\n",
      "[[-79 -77 -79 ... -71 -72 -73]\n",
      " [-73 -70 -68 ... -59 -54 -54]\n",
      " [-54 -51 -48 ... -82 -84 -85]\n",
      " [-84 -83 -83 ... -68 -67 -70]\n",
      " [-71 -69 -69 ... -64 -62 -65]]\n",
      "(9346, 5000)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 3 3 3]\n",
      " [3 3 3 ... 0 0 0]\n",
      " [0 0 0 ... 3 3 3]\n",
      " [3 3 3 ... 0 0 0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'onehot_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(y[:\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43monehot_labels\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'onehot_labels' is not defined"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[:5])\n",
    "print(y.shape)\n",
    "print(y[:5])\n",
    "print(onehot_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6542, 5000)\n",
      "(6542, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape X to add channel dimension\n",
    "X1 = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6542, 5000, 1)\n",
      "(2804, 5000, 1)\n",
      "(6542, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model definition\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(64, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(64, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')   \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target data (y_train) should be a 1D array of shape (None,) for sparse categorical crossentropy.\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 5000), output.shape=(None, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PG_Github/MFA-matpy/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/PG_Github/MFA-matpy/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py:652\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m     )\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m     )\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 5000), output.shape=(None, 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 2804\n'y' sizes: 14020000\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/PG_Github/MFA-matpy/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/PG_Github/MFA-matpy/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:114\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    110\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 2804\n'y' sizes: 14020000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (2804, 5000, 1)\n",
      "y_test shape: (14020000,)\n",
      "x: <class 'numpy.ndarray'>\n",
      "y: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print('x:', type(X_test))\n",
    "print('y:', type(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py_ecg)",
   "language": "python",
   "name": "py_ecg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
